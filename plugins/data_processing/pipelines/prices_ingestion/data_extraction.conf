google_region = us
google_region = ${?GOOGLE_REGION}

bucket = ${GOOGLE_PROJECT_ID}-${google_region}-dcm-data-test
prefix = "chris/"
eventlet_patcher_flag = false
fluent_host = localhost
fluent_host = ${?FLUENT_HOST}
fluent_port = 24224
fluent_port = ${?FLUENT_PORT}
dcm_environment = "test"
data_lake_bucket = ${GOOGLE_PROJECT_ID}-dcm-data-lake
data_processing_bucket = ${GOOGLE_PROJECT_ID}-dcm-data-processing
data_results_bucket = ${GOOGLE_PROJECT_ID}-dcm-data-results

kafka {
    bootstrap_servers = broker-0.kafka.aws:9092
    sasl_plain_username = ""
    sasl_plain_password = ""
    ssl_cafile = ""
    ssl_certfile = ""
    ssl_keyfile = ""

    bootstrap_servers = ${?KAFKA_BOOTSTRAP}
    sasl_plain_username = ${?KAFKA_SASL_PLAIN_USERNAME}
    sasl_plain_password = ${?KAFKA_SASL_PLAIN_PASSWORD}
    ssl_cafile = ${?KAFKA_SSL_CAFILE}
    ssl_certfile = ${?KAFKA_SSL_CERTFILE}
    ssl_keyfile = ${?KAFKA_SSL_KEYFILE}
}

logging {
  version = 1
  disable_existing_loggers = false
  formatters {
    time_and_message {
      format = "%(asctime)s\t%(levelname)-8s\t%(message)s"
    }
    fluent_fmt {
      "()" = fluent.handler.FluentRecordFormatter
      format {
        level = "%(levelname)s"
        hostname = "%(hostname)s"
        where = "%(module)s.%(funcName)s"
        task = "%(task)s",
        stack_trace = "%(exc_text)s"
        date = "%(date)s"
        ticker = "%(ticker)s"
      }
    }
    stage_log_fmt {
      "()" = fluent.handler.FluentRecordFormatter
      format {
        level = "%(levelname)s"
        hostname = "%(hostname)s"
        date = "%(date)s"
        ticker = "%(ticker)s"
      }
    }
    fluent_fmt_nodate {
      "()" = fluent.handler.FluentRecordFormatter
      format {
        level = "%(levelname)s"
        hostname = "%(hostname)s"
        where = "%(module)s.%(funcName)s"
        stack_trace = "%(exc_text)s"
      }
    }
    fluent_qa {
      "()" = fluent.handler.FluentRecordFormatter
      format {
        level = "%(levelname)s"
        hostname = "%(hostname)s"
        from = "%(module)s"
        date = "%(date)s"
      }
    }
  }
  handlers {
    console {
      class = logging.StreamHandler
      formatter = time_and_message
      stream = "ext://sys.stderr"
      level = INFO
    }
    fluent {
      class = fluent.handler.FluentHandler
      formatter = fluent_fmt
      host = ${fluent_host}
      port = ${fluent_port}
      tag = intuition.data_extraction
      level = INFO
    }
    stage_log {
      class = fluent.handler.FluentHandler
      formatter = stage_log_fmt
      host = ${fluent_host}
      port = ${fluent_port}
      tag = stage_log
      level = INFO
    }
    fluent_adjustment_and_errors {
      class = fluent.handler.FluentHandler
      formatter = fluent_fmt
      host = ${fluent_host}
      port = ${fluent_port}
      tag = intuition.data_extraction.adjustment
      level = INFO
    }
    fluent_nodate {
      class = fluent.handler.FluentHandler
      formatter = fluent_fmt_nodate
      host = ${fluent_host}
      port = ${fluent_port}
      tag = intuition.data_extraction.adjustment
      level = INFO
    }
    fluent_qa {
      class = fluent.handler.FluentHandler
      formatter = fluent_qa
      host = ${fluent_host}
      port = ${fluent_port}
      tag = intuition.data_extraction.qa
      level = INFO
    }
  }
  loggers {
    adjustments_logger {
      level = INFO
      handlers = [console]
      propagate = false
    }
    adjustment_and_errors_logger {
      level = INFO
      handlers = [console]
    }
    root {
      level = INFO
      handlers = [console, fluent_nodate]
    }
    data_qa {
      level = INFO
      handlers = [console, fluent_qa]
      propagate = false
    }
    data_extraction {
      level = INFO
      handlers = [console, fluent]
    }
    stage_log {
      level = INFO
      handlers = [console, stage_log]
    }
  }
}

raw_data_pull {
  base_data_bucket = ${bucket}
  Cboe {
    Index {
      url_base = "http://www.cboe.com/publish/ScheduledTask/MktData/datahouse/"
      data_location_prefix = ${prefix}cboe_index
    }
  }
  TickData {
    Price {
      data_location_prefix = ${prefix}tickdata
      URL = "https://tickapi.tickdata.com/tickmarket?COLLECTION_TYPE=US_TED&EXTRACT_TYPE=TIMEBAR&START_DATE={start}"
	  RedshiftLoad {
	    RoleArn = "arn:aws:iam::294659765478:role/slave"
		RoleSessionName = "AdjustmentImportDaily"
	  }
	  include "credentials.conf"
    }
    CA {
      data_pull_URL = "https://tickapi.tickdata.com/events?COLLECTION_TYPE=COLLECTION_TYPE.US_TED&EXTRACT_TYPE=COLLECTOR_SUBTYPE_US_EVENTS&START_DATE={start}&END_DATE={end}&REQUESTED_DATA={ticker}&COMPRESSION_TYPE=NONE"
      data_location_prefix = ${prefix}tickdata_ca
      defaults {
        start = "01/02/1993"
      }
    }
  }
  CapIQ {
    EarningsCalendar {
      data_location_prefix = ${prefix}capiq_data
      filename_root = earnings_calendar
      }
  }
  Yahoo {
    PriceAndCA {
      price_data_location_prefix = ${prefix}yahoo_data/price_data
      ca_data_location_prefix = ${prefix}yahoo_data/corporate_actions
      base_page_URL = "https://finance.yahoo.com/quote/{ticker}/history?p={ticker}"
      data_pull_URL = "https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={start_dt_ts}&period2={end_dt_ts}&interval=1d&crumb={crumb}"
      defaults {
        start_dt = "1900-01-01"
      }
    }
    EarningsCalendar {
      data_location_prefix = ${prefix}yahoo_data
      filename_root = earnings_calendar
      URL = "http://biz.yahoo.com/research/earncal/{date}.html"
      universe_file_location = "chris/input/"
      universe_file_name = "full_universe_yahoo.csv"
    }
  }
  TickDataH5 {
	Price {
		data_location = "C:/DCM/temp/tickdata_raw"
	}
  }
  #Dragonfish {
    #ftp {
      #host = "52.36.34.97"
      #user = "DCM"
      #password = "test_pwd"
      #password = ${?DRAGONFISH_FTP_PWD}
      #directory = "/"
    #}
    #data_location_prefix = ${prefix}dragonfish
  #}
}

adjustments {
  # The following can all optionally be set via the environment variables on the right hand side,
  # if they are not present, these fields will not be defined and you have to configure them in your other
  # configuration overrides
  partitions=40
  partitions=${?ADJUSTMENT_PARTITIONS}
  # -1 means keep running forever
  stop_running_after=12
  stop_running_after=${?STOP_RUNNING_AFTER}
  first_relevant_date_for_all_tests = "2000-01-03"
  as_of_date = ${?ADJUSTMENT_AS_OF_DATE}
  data_start = "2000-01-03"
  data_start = ${?ADJUSTED_DATA_START}
  data_end = ${?ADJUSTED_DATA_END}
  data_bucket = ${bucket}
  data_location_base = "fluder/price_ingestion/adjusted_data"
  temp_location = ${adjustments.data_location_base}"/.temp"
  manifest_location = ${adjustments.data_location_base}"/.manifests"
  pickle_location = ${adjustments.data_location_base}"/.pickle"
  data_prefix = ${adjustments.data_location_base}"/price_data_20170707"
  daily_data_prefix = ${adjustments.data_location_base}"/daily_price_data"
  adjustment_factors = ${adjustments.data_location_base}"/adjustment_factors"
  pipeline_status_table = "pipeline_status"
  rth_start = "09:31"
  rth_end = "16:00"
  bfill_limit_minutes = 3
  normalization_cutoff = "2010-01-04"
  first_verification_date_on_database = "2016-05-01"
  price_adjustment_verification_threshold = 0.0005
  max_missing_tickers = 1%
  verification_parameters {
    minute_outlier {
      jumps_tolerance_open_to_close = [0.2, 5]
      volume_comparison_running_window = 30
      outlier_threshold_stds = 3
      threshold_vol_reference = 0.015
      outlier_price_limit = 0.01
	  open_mismatch_tolerance = 0.003
	  close_mismatch_tolerance = 0.003
    }
    daily_outlier {
      jumps_tolerance_open_to_close = [0.2, 5]
      volume_comparison_running_window = 30
      daily_outlier_threshold_stds = 3
      low_daily_outlier_price_limit = -0.1
      upper_daily_outlier_price_limit = 0.1
    }
    equity_data_holder_tests {
      min_tolerance_change_ratio = 0.2
      max_tolerance_change_ratio = 5
      min_price_check_amount = 0.00
    }
    corporate_action_outlier {
      outlier_price_limit = 0.02
    }
    timeline_verification {
      cut_off_date_for_missing_data = "2004-01-01"
    }
  }
  s3_yahoo_price_data_read {
    date_yahoo_changed_file_format = "2017-05-13"
  }
}

consolidated_earnings {
  data_location_prefix = ${prefix}consolidated
  filename_root = earnings_calendar
  backward_time_offset = "22B"
  forward_time_offset = "65B"

  temp_table_id = ${gcp.default_dataset}.earnings_staging
  dest_table_id = ${gcp.default_dataset}.earnings
}

calibration {
    max_missing_tickers_allowed = 2.5%
    max_missing_tickers_allowed = ${?MAX_MISSING_TICKERS}
}

gcp {
    project_id = ${GOOGLE_PROJECT_ID}
    default_dataset = marketdata
    default_dataset = ${?BQ_DEFAULT_DATASET}
}

redshift {
  host = "marketdata-test.cq0v4ljf3cuw.us-east-1.redshift.amazonaws.com"
  # host = "test-env-cluster.cq0v4ljf3cuw.us-east-1.redshift.amazonaws.com"
  dbname = "dev"
  port = 5439
  username = ${?REDSHIFT_USER}
  password = ${?REDSHIFT_PWD}
  equity_price_table = ${gcp.default_dataset}.equity_prices
  raw_tickdata_equity_price_table = ${gcp.default_dataset}.raw_equity_prices
  daily_price_table = ${gcp.default_dataset}.daily_equity_prices
  adjustment_factors_table = ${gcp.default_dataset}.equity_adjustment_factors
  whitelist_table = ${gcp.default_dataset}.whitelist
}

security_master {
  dcm_config_repo_path = "/opt/dcm-config"
  dcm_config_branch = "master"  # "updating_universe_file"
  security_universe_table = "security_universe"  # "security_universe_copy"
  pending_security_changes_table = "pending_security_changes"  # in postgres
}

global_trader_params {
  earnings_horizon = "10B"
  stop_loss = -0.5
  stop_gain = 0.05
}

supervisor_params {
  total_funding = 2500000.0
  fundingRequestsAmount = 25000.0
  funding_buffer = 15000.0
}

file_locations {
  calibration_bucket = ${bucket}
  calibration_prefix = ${prefix}"calibrations/earnings/"
  out_dir = "./output/"
  out_dir = ${?OUT_DIR}
  category_file = "rp_category_lookup.csv.bz2"
}

pre_earnings {
  schema_name = "pre_earnings_"${dcm_environment}
  latest_enriched = "latest_enriched"
  latest_digitized = "latest_digitized"
}

etl_equities_process_labels = {
  LOCATION_S3_YAHOO_PRICE_LABEL = "YahooPriceBucketOnS3"
  LOCATION_S3_YAHOO_CORPORATE_ACTION_LABEL = "YahooCABucketOnS3"
  LOCATION_S3_TICKDATA_PRICE_LABEL = "TickDataPriceBucketOnS3"
  LOCATION_S3_TICKDATA_PRICE_MANIFEST_FILE_LABEL = "TickDataManifest"
  LOCATION_S3_TICKDATA_CA_FILE_LABEL = "TickDataCorporateActionsOnS3"
  LOCATION_S3_ADJUSTED_DATA = "AdjustedDataBucketOnS3"
  LOCATION_S3_ADJUSTED_DAILY_DATA = "AdjustedDailyDataBucketOnS3"
  S3_ADJ_FACTORS_LOCATION_LABEL = "S3_AdjFactorsLocation"

  DATA_YAHOO_PRICE_LABEL = "YahooPrices"
  DATA_TICKDATA_PRICE_HANDLER_LABEL = "TickDataPrice"
  DATA_YAHOO_CORPORATE_ACTION_LABEL = "YahooCA"
  DATA_TICKDATA_ONEDAY_PRICE_LABEL = "TickDataOneDayPrice"

  DATA_LAST_ADJUSTMENT_DATES = "RedshiftLastAdjustmentDates"
  DATA_LAST_CORPORATE_ACTIONS_DATES = "YahooLastCADates"
  DATA_LAST_CORPORATE_ACTIONS_TICKDATA_DATES = "TickDataLastCADates"
  DATA_YAHOO_CORPORATE_ACTION_PREVIOUS_DATE_LABEL = "YahooCAPreviousDay"
  DATA_YAHOO_CORPORATE_ACTION_FIRST_DATE_LABEL = "YahooCAFirstAvailableDay"

  DATA_YAHOO_PRICE_PREVIOUS_DATE_LABEL = "YahooPricePreviousDay"
  TICKDATA_CORPORATE_ACTION_LABEL = "TickDataCA"
  TICKDATA_CORPORATE_ACTION_PREVIOUS_DATE_LABEL = "TickDataCAPreviousDay"
  TICKDATA_CORPORATE_ACTION_FIRST_DATE_LABEL = "TickDataCAFirstAvailableDay"
  
  TICKDATA_TICKER_HISTORY_LABEL = "TickDataTickerHistory"
}

etl_equities_results_labels = {
  QA_DATA_TO_CHECK = "TestData"
  QA_REFERENCE_DATA = "ReferenceData"
  S3_LOCATION_LABEL = "S3_Location"

  DATA_LINEAGE_LABEL = "DataLineage"
  DATA_HOLDER_TYPE_LABEL = "DataHolderType"

  REPORT_ERROR_LABEL = "ProcessErrors"
  REPORT_WARNING_LABEL = "ProcessWarnings"
  REPORT_DESCRIPTION_LABEL = "Description"
  REPORT_DETAILS_LABEL = "Details"
}

etl_earnings_calendar_results_labels = {
  RAW_YAHOO_DATA_LABEL = "RawEarningsCalendarYahoo"
  CLEAN_YAHOO_DATA_LABEL = "CleanEarningsCalendarYahoo"
  RAW_CAPIQ_DATA_LABEL = "RawEarningsCalendarCapIQ"
  CLEAN_CAPIQ_DATA_LABEL = "CleanEarningsCalendarCapIQ"
  CONSOLIDATED_DATA_LABEL = "ConsolidatedEarningsCalendar"
}

ParallelQueryRedshiftDownload {
  s3_default_location = "s3://dcm-test/data-pipelines"
  credentials {
    s3 {
      access_key = ${?AWS_ACCESS_KEY_ID}
      secret_key = ${?AWS_SECRET_ACCESS_KEY}
    }
    redshift {
	host = marketdata-test.cq0v4ljf3cuw.us-east-1.redshift.amazonaws.com
	username = ${?REDSHIFT_USER}
	password = ${?REDSHIFT_PWD}
	database = dev
	port = 5439
    }
  }
}

SQLDataProvider: {
    parallelize_queries = False,
    parallel_query_downloader_kwargs = {}
}
 
DataProvider.RedshiftRavenPackDataProvider {
  redshift_user = dcm_dev
  redshift_user = ${?REDSHIFT_USER}
  redshift_pwd = pwd
  redshift_pwd = ${?REDSHIFT_PWD}
  redshift_endpoint = marketdata-test.cq0v4ljf3cuw.us-east-1.redshift.amazonaws.com
  redshift_port = 5439
  db_name = dev
  redshift_credentials = ${DataProvider.RedshiftRavenPackDataProvider.redshift_user}:${DataProvider.RedshiftRavenPackDataProvider.redshift_pwd}
  redshift_service = ${DataProvider.RedshiftRavenPackDataProvider.redshift_endpoint}:${DataProvider.RedshiftRavenPackDataProvider.redshift_port}
  redshift_uri = "redshift+psycopg2://"${DataProvider.RedshiftRavenPackDataProvider.redshift_credentials}@${DataProvider.RedshiftRavenPackDataProvider.redshift_service}/${DataProvider.RedshiftRavenPackDataProvider.db_name}
}

DataProvider.HistoricalDataProviders {
  redshift_uri = ${DataProvider.RedshiftRavenPackDataProvider.redshift_uri}
}
