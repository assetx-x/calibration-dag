steps:
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        gcloud -q auth activate-service-account --key-file=${_SA_KEY}
    id: 'auth'
    volumes:
      - name: 'shared-data'
        path: '/data'

  - name: 'gcr.io/cloud-builders/gsutil'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        gsutil -m cp -r dags/* gs://us-east4-airflowcomposer-d2353cf2-bucket/dags/
        gsutil -m cp -r plugins/* gs://us-east4-airflowcomposer-d2353cf2-bucket/plugins/
        gsutil -m cp -r pip.conf gs://us-east4-airflowcomposer-d2353cf2-bucket/conf/
    id: 'upload-all'
    waitFor: ['auth']
    volumes:
      - name: 'shared-data'
        path: '/data'

  - name: 'custom-python-image' # replace with your custom image name
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        python -m pip download -r requirements-light.txt -d /data/dist
        python -m twine upload -v --repository-url https://us-east4-python.pkg.dev/dcm-dev-9fe8/python-repo/ /data/dist/*.whl --skip-existing
    id: 'download-and-upload-to-artifact-registry'
    env:
      - PIP_ROOT_USER_ACTION=ignore
      - GOOGLE_APPLICATION_CREDENTIALS=${_SA_KEY}
      - PIP_CONFIG_FILE=pip.conf
    waitFor: ['upload-all']
    volumes:
      - name: 'shared-data'
        path: '/data'

  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        gcloud composer environments storage dags import --environment dcm-dev-9fe8 --location us-east4 --source gs://us-east4-airflowcomposer-d2353cf2-bucket/dags/
        gcloud composer environments storage plugins import --environment dcm-dev-9fe8 --location us-east4 --source gs://us-east4-airflowcomposer-d2353cf2-bucket/plugins/
        gcloud composer environments update dcm-dev-9fe8 --location us-east4 --update-pypi-packages-from-file gs://us-east4-airflowcomposer-d2353cf2-bucket/conf/pip.conf
    id: 'import-and-update'
    timeout: 600s
    waitFor: ['download-and-upload-to-artifact-registry']
    volumes:
      - name: 'shared-data'
        path: '/data'

options:
  logging: CLOUD_LOGGING_ONLY